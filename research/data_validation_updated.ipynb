{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251f6620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f216eca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e5fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\deepu\\OneDrive\\Desktop\\ML_PIPELINE')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d845f9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb56b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"artifacts/data_ingestion/winequality-red.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac39286",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6daa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3be5673",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce03683",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(data.dtypes)['fixed acidity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4586cbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f016750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87ba9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataValidationConfig:\n",
    "    root_dir: Path\n",
    "    STATUS_FILE: str\n",
    "    unzip_data_dir: Path\n",
    "    all_schema: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd059ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datascience.constants import *\n",
    "from src.datascience.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e1c733",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "            self,\n",
    "            config_filepath = CONFIG_FILE_PATH,\n",
    "            params_filepath = PARAMS_FILE_PATH,\n",
    "            schema_filepath = SCHEMA_FILE_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "    \n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    def get_data_validation_config(self) -> DataValidationConfig:\n",
    "        config = self.config.data_validation\n",
    "        schema = self.schema.COLUMNS\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_validation_config = DataValidationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            STATUS_FILE=config.STATUS_FILE,\n",
    "            unzip_data_dir=config.unzip_dir,\n",
    "            all_schema=schema\n",
    "        )\n",
    "        return data_validation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f188c3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.datascience import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation_class_header",
   "metadata": {},
   "source": [
    "## Updated DataValidation Class with Datatype Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90094717",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataValidation:\n",
    "    def __init__(self, config: DataValidationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    \n",
    "    def validate_all_columns(self) -> bool:\n",
    "        \"\"\"\n",
    "        Validates if all required columns exist in the dataset\n",
    "        \"\"\"\n",
    "        try:\n",
    "            validation_status = None\n",
    "\n",
    "            data = pd.read_csv(self.config.unzip_data_dir)\n",
    "            all_cols = list(data.columns)\n",
    "\n",
    "            all_schema = self.config.all_schema.keys()\n",
    "\n",
    "            # Check if all columns from data exist in schema\n",
    "            for col in all_cols:\n",
    "                if col not in all_schema:\n",
    "                    validation_status = False\n",
    "                    with open(self.config.STATUS_FILE, 'w') as f:\n",
    "                        f.write(f\"Column Validation Status: {validation_status}\\n\")\n",
    "                        f.write(f\"Missing column in schema: {col}\\n\")\n",
    "                    logger.error(f\"Column '{col}' not found in schema\")\n",
    "                    return validation_status\n",
    "            \n",
    "            # All columns exist\n",
    "            validation_status = True\n",
    "            with open(self.config.STATUS_FILE, 'w') as f:\n",
    "                f.write(f\"Column Validation Status: {validation_status}\\n\")\n",
    "            \n",
    "            logger.info(\"All columns validated successfully\")\n",
    "            return validation_status\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "\n",
    "    def validate_all_dtypes(self) -> bool:\n",
    "        \"\"\"\n",
    "        Validates if all column datatypes match the schema\n",
    "        \"\"\"\n",
    "        try:\n",
    "            validation_status = None\n",
    "            \n",
    "            # Read the data\n",
    "            data = pd.read_csv(self.config.unzip_data_dir)\n",
    "            \n",
    "            # Get actual datatypes from CSV\n",
    "            actual_dtypes = data.dtypes\n",
    "            \n",
    "            # Get expected datatypes from schema\n",
    "            expected_schema = self.config.all_schema\n",
    "            \n",
    "            # Open file in append mode to add dtype validation results\n",
    "            with open(self.config.STATUS_FILE, 'a') as f:\n",
    "                f.write(f\"\\n{'='*50}\\n\")\n",
    "                f.write(f\"Datatype Validation:\\n\")\n",
    "                f.write(f\"{'='*50}\\n\")\n",
    "            \n",
    "            # Compare each column's datatype\n",
    "            mismatches = []\n",
    "            for col in data.columns:\n",
    "                actual_dtype = str(actual_dtypes[col])\n",
    "                expected_dtype = str(expected_schema[col])\n",
    "                \n",
    "                if actual_dtype != expected_dtype:\n",
    "                    mismatch_msg = f\"  ‚ùå {col}: Expected '{expected_dtype}', Got '{actual_dtype}'\"\n",
    "                    mismatches.append(mismatch_msg)\n",
    "                    logger.warning(mismatch_msg)\n",
    "            \n",
    "            # Write results\n",
    "            if len(mismatches) > 0:\n",
    "                validation_status = False\n",
    "                with open(self.config.STATUS_FILE, 'a') as f:\n",
    "                    f.write(f\"Datatype Validation Status: {validation_status}\\n\")\n",
    "                    f.write(f\"Mismatches Found:\\n\")\n",
    "                    for mismatch in mismatches:\n",
    "                        f.write(f\"{mismatch}\\n\")\n",
    "                logger.error(f\"Found {len(mismatches)} datatype mismatches\")\n",
    "            else:\n",
    "                validation_status = True\n",
    "                with open(self.config.STATUS_FILE, 'a') as f:\n",
    "                    f.write(f\"Datatype Validation Status: {validation_status}\\n\")\n",
    "                    f.write(f\"‚úÖ All datatypes match the schema!\\n\")\n",
    "                logger.info(\"All datatypes validated successfully\")\n",
    "            \n",
    "            return validation_status\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "execution_header",
   "metadata": {},
   "source": [
    "## Execute Data Validation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b91050",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_validation_config = config.get_data_validation_config()\n",
    "    data_validation = DataValidation(config=data_validation_config)\n",
    "    \n",
    "    # Step 1: Validate column names\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Step 1: Validating Column Names\")\n",
    "    print(\"=\"*60)\n",
    "    column_status = data_validation.validate_all_columns()\n",
    "    logger.info(f\"Column validation status: {column_status}\")\n",
    "    print(f\"‚úÖ Column Validation: {'PASSED' if column_status else 'FAILED'}\")\n",
    "    \n",
    "    # Step 2: Validate datatypes\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Step 2: Validating Column Datatypes\")\n",
    "    print(\"=\"*60)\n",
    "    dtype_status = data_validation.validate_all_dtypes()\n",
    "    logger.info(f\"Datatype validation status: {dtype_status}\")\n",
    "    print(f\"‚úÖ Datatype Validation: {'PASSED' if dtype_status else 'FAILED'}\")\n",
    "    \n",
    "    # Overall validation\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Final Results\")\n",
    "    print(\"=\"*60)\n",
    "    if column_status and dtype_status:\n",
    "        logger.info(\"‚úÖ All validations passed!\")\n",
    "        print(\"üéâ ALL VALIDATIONS PASSED! üéâ\")\n",
    "        print(f\"\\nDetailed report saved at: {data_validation_config.STATUS_FILE}\")\n",
    "    else:\n",
    "        logger.warning(\"‚ö†Ô∏è Some validations failed. Check status.txt for details.\")\n",
    "        print(\"‚ö†Ô∏è SOME VALIDATIONS FAILED\")\n",
    "        print(f\"\\nCheck details at: {data_validation_config.STATUS_FILE}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERROR OCCURRED: {str(e)}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "view_status",
   "metadata": {},
   "source": [
    "## View Validation Status File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "view_status_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and display the status file\n",
    "with open('artifacts/data_validation/status.txt', 'r') as f:\n",
    "    print(f.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
